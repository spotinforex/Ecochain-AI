{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1318ae-77d1-4bb9-bca0-272a2f7f15fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor \n",
    "from lightgbm import LGBMRegressor \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor \n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import optuna\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error as MAE, r2_score as R2\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e8f076-ecd3-459e-bad4-8a61fc81639b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supplier_id</th>\n",
       "      <th>supplier_name</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>product_category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>total_eco_score</th>\n",
       "      <th>carbon_score</th>\n",
       "      <th>water_score</th>\n",
       "      <th>waste_score</th>\n",
       "      <th>...</th>\n",
       "      <th>partnership_status</th>\n",
       "      <th>annual_volume</th>\n",
       "      <th>cost_premium</th>\n",
       "      <th>risk_level</th>\n",
       "      <th>last_audit</th>\n",
       "      <th>audit_summary</th>\n",
       "      <th>image_url</th>\n",
       "      <th>recommendation</th>\n",
       "      <th>text_embedding</th>\n",
       "      <th>text_embedding_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>SUP3720</td>\n",
       "      <td>Wade, Black and York</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Food</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>0.96</td>\n",
       "      <td>92.30</td>\n",
       "      <td>18.50</td>\n",
       "      <td>58.05</td>\n",
       "      <td>...</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>745347</td>\n",
       "      <td>13.74</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>ISO14001 compliance confirmed; Rainforest Alli...</td>\n",
       "      <td>gs://ecochain-product-images/sugar.jpeg</td>\n",
       "      <td>Avoid</td>\n",
       "      <td>[-0.0738530233502388, -0.04022129997611046, -0...</td>\n",
       "      <td>[-0.03160709887742996, -0.01454420667141676, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>SUP1809</td>\n",
       "      <td>Jones, Gonzalez and Garza</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Food</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>83.91</td>\n",
       "      <td>6.03</td>\n",
       "      <td>2.55</td>\n",
       "      <td>64.93</td>\n",
       "      <td>...</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>160009</td>\n",
       "      <td>18.11</td>\n",
       "      <td>High</td>\n",
       "      <td>2023-08-21</td>\n",
       "      <td>ISO14001 compliance confirmed; GOTS standards ...</td>\n",
       "      <td>gs://ecochain-product-images/sugar.jpeg</td>\n",
       "      <td>Preferred</td>\n",
       "      <td>[-0.3574414849281311, 0.017862210050225258, -0...</td>\n",
       "      <td>[-0.9049476981163025, 0.22246497869491577, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     supplier_id              supplier_name  country region product_category  \\\n",
       "4998     SUP3720       Wade, Black and York  Vietnam   Asia             Food   \n",
       "4999     SUP1809  Jones, Gonzalez and Garza  Vietnam   Asia             Food   \n",
       "\n",
       "     sub_category  total_eco_score  carbon_score  water_score  waste_score  \\\n",
       "4998        Sugar             0.96         92.30        18.50        58.05   \n",
       "4999        Sugar            83.91          6.03         2.55        64.93   \n",
       "\n",
       "      ...  partnership_status annual_volume cost_premium  risk_level  \\\n",
       "4998  ...        Under Review        745347        13.74      Medium   \n",
       "4999  ...        Under Review        160009        18.11        High   \n",
       "\n",
       "      last_audit                                      audit_summary  \\\n",
       "4998  2022-02-10  ISO14001 compliance confirmed; Rainforest Alli...   \n",
       "4999  2023-08-21  ISO14001 compliance confirmed; GOTS standards ...   \n",
       "\n",
       "                                    image_url recommendation  \\\n",
       "4998  gs://ecochain-product-images/sugar.jpeg          Avoid   \n",
       "4999  gs://ecochain-product-images/sugar.jpeg      Preferred   \n",
       "\n",
       "                                         text_embedding  \\\n",
       "4998  [-0.0738530233502388, -0.04022129997611046, -0...   \n",
       "4999  [-0.3574414849281311, 0.017862210050225258, -0...   \n",
       "\n",
       "                                       text_embedding_2  \n",
       "4998  [-0.03160709887742996, -0.01454420667141676, -...  \n",
       "4999  [-0.9049476981163025, 0.22246497869491577, -0....  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('supplier_embeddings.csv')\n",
    "data.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37673ef1-61c7-4e47-87cd-0bac7b57a4a3",
   "metadata": {},
   "source": [
    "Dropping Columns already Embedded and those not needed for testing (View the other notebook to see when the embedding was done in BigQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b518060a-340e-4be9-9f9b-6a67d50c4085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['supplier_id', 'supplier_name', 'country', 'region', 'product_category',\n",
       "       'sub_category', 'total_eco_score', 'carbon_score', 'water_score',\n",
       "       'waste_score', 'social_score', 'certification', 'partnership_status',\n",
       "       'annual_volume', 'cost_premium', 'risk_level', 'last_audit',\n",
       "       'audit_summary', 'image_url', 'recommendation', 'text_embedding',\n",
       "       'text_embedding_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78bcbfb1-1057-4ee8-9b64-18e2ad4539bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['supplier_id','supplier_name','audit_summary','image_url','certification','text_embedding_2','sub_category','product_category', 'recommendation'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1474c826-6e39-4c24-adad-91f27c5ef891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>total_eco_score</th>\n",
       "      <th>carbon_score</th>\n",
       "      <th>water_score</th>\n",
       "      <th>waste_score</th>\n",
       "      <th>social_score</th>\n",
       "      <th>partnership_status</th>\n",
       "      <th>annual_volume</th>\n",
       "      <th>cost_premium</th>\n",
       "      <th>risk_level</th>\n",
       "      <th>last_audit</th>\n",
       "      <th>text_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>Americas</td>\n",
       "      <td>97.79</td>\n",
       "      <td>91.94</td>\n",
       "      <td>65.86</td>\n",
       "      <td>34.97</td>\n",
       "      <td>83.13</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>697830</td>\n",
       "      <td>17.2</td>\n",
       "      <td>Low</td>\n",
       "      <td>2025-06-17</td>\n",
       "      <td>[-0.3144732713699341, -0.051802970468997955, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     country    region  total_eco_score  carbon_score  water_score  \\\n",
       "0  Argentina  Americas            97.79         91.94        65.86   \n",
       "\n",
       "   waste_score  social_score partnership_status  annual_volume  cost_premium  \\\n",
       "0        34.97         83.13           Inactive         697830          17.2   \n",
       "\n",
       "  risk_level  last_audit                                     text_embedding  \n",
       "0        Low  2025-06-17  [-0.3144732713699341, -0.051802970468997955, 0...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec01bd-c40b-4a28-bbec-eeb65d81d088",
   "metadata": {},
   "source": [
    "Splitting the Data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60fe2338-25d5-4b46-8f34-fe7da0ecc45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['carbon_score','water_score','waste_score','social_score','total_eco_score'], axis = 1)\n",
    "y = data[['carbon_score','water_score','waste_score','social_score']]\n",
    "y_ecoscore = data['total_eco_score']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fedffd55-52ba-40dd-971e-026cb04ada1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>partnership_status</th>\n",
       "      <th>annual_volume</th>\n",
       "      <th>cost_premium</th>\n",
       "      <th>risk_level</th>\n",
       "      <th>last_audit</th>\n",
       "      <th>recommendation</th>\n",
       "      <th>text_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4227</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>948098</td>\n",
       "      <td>4.09</td>\n",
       "      <td>High</td>\n",
       "      <td>2023-05-05</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[-0.12168096005916595, 0.015789248049259186, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           country  region partnership_status  annual_volume  cost_premium  \\\n",
       "4227  South Africa  Africa           Inactive         948098          4.09   \n",
       "\n",
       "     risk_level  last_audit recommendation  \\\n",
       "4227       High  2023-05-05        Neutral   \n",
       "\n",
       "                                         text_embedding  \n",
       "4227  [-0.12168096005916595, 0.015789248049259186, 0...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4541bb4-9f4e-4ef7-9c2d-15b460fe8d10",
   "metadata": {},
   "source": [
    "Encoding Categorical Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c7e9b21-b40e-48d8-9c54-ccc8f2a12fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()\n",
    "\n",
    "X_train[['country','region','partnership_status','risk_level']] = encoder.fit_transform(X_train[['country','region','partnership_status','risk_level']])\n",
    "\n",
    "X_test[['country','region','partnership_status','risk_level']] = encoder.transform(X_test[['country','region','partnership_status','risk_level']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c664a07c-f250-44e7-abdf-7f850f74bf28",
   "metadata": {},
   "source": [
    "Removing the dash in the last audit column to make them pure numbers and turning the text embedding into a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af4fafe8-9dfb-4e34-8545-a14fa6c70a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "X_train['last_audit'] = X_train['last_audit'].str.replace(\"-\",\"\", regex = False)\n",
    "X_test['last_audit'] = X_test['last_audit'].str.replace(\"-\",\"\", regex = False)\n",
    "\n",
    "X_train['last_audit'] = pd.to_numeric(X_train['last_audit'], errors = 'coerce')\n",
    "X_test['last_audit'] = pd.to_numeric(X_test['last_audit'], errors = 'coerce')\n",
    "\n",
    "X_train['text_embedding'] = [ast.literal_eval(e) for e in X_train['text_embedding']]\n",
    "X_test['text_embedding'] = [ast.literal_eval(e) for e in X_test['text_embedding']]\n",
    "\n",
    "X_train['text_embedding'] = [[float(x) for x in row] for row in X_train['text_embedding']]\n",
    "X_test['text_embedding'] = [[float(x) for x in row] for row in X_test['text_embedding']]\n",
    "\n",
    "# Convert embeddings column (list of floats) into a 2D NumPy array\n",
    "X_train_embeddings = np.vstack(X_train['text_embedding'].values).astype(np.float32)\n",
    "X_test_embeddings = np.vstack(X_test['text_embedding'].values).astype(np.float32)\n",
    "\n",
    "# Drop the original embedding column from DataFrame\n",
    "X_train_num = X_train.drop(columns=['text_embedding']).reset_index(drop=True)\n",
    "X_test_num = X_test.drop(columns=['text_embedding']).reset_index(drop=True)\n",
    "\n",
    "# Concatenate numeric features + embeddings into final arrays\n",
    "X_train_final = np.hstack([X_train_num.values, X_train_embeddings])\n",
    "X_test_final = np.hstack([X_test_num.values, X_test_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5145748b-7be4-4df3-bfcf-46d01539c14e",
   "metadata": {},
   "source": [
    "First Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce384ddd-022e-466d-b406-2b0e9991c6b8",
   "metadata": {},
   "source": [
    "Training BaseLine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e89d3e6-ae55-46ff-8f58-dd84b43fa17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 28.87\n",
      "Baseline MAE: 25.01\n",
      "Baseline R2: -0.00\n"
     ]
    }
   ],
   "source": [
    "dummy_model = DummyRegressor(strategy = 'mean')\n",
    "dummy_model.fit(X_train_final, y_train)\n",
    "\n",
    "\n",
    "ypred = dummy_model.predict(X_test_final)\n",
    "\n",
    "baseline_test = pd.DataFrame(ypred)\n",
    "baseline_rmse = np.sqrt(MSE(y_test,baseline_test))\n",
    "baseline_mae = MAE(y_test,baseline_test)\n",
    "baseline_r2 = R2(y_test,baseline_test)\n",
    "print(f'Baseline RMSE: {baseline_rmse:.2f}')\n",
    "print(f'Baseline MAE: {baseline_mae:.2f}')\n",
    "print(f'Baseline R2: {baseline_r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2876bbb-b6fe-4528-aaa6-5ddcb4d7ab39",
   "metadata": {},
   "source": [
    "The Base Line RMSE of the ecochain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3435eda7-fc07-4fa8-b637-ec10b67194ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "Model xgboost Training In Progess\n",
      "Training Model xgboost Complete\n",
      "Prediction For Sub Scores In Progress\n",
      "SubScores:\n",
      " RMSE FOR carbon_score: Training Score: 20.007210321519665 Testing Score: 26.46369938445744\n",
      " MAE FOR carbon_score: Training Score: 16.83710017732382 Testing Score: 22.506502940387726\n",
      " r2 FOR carbon_score: Training Score: 0.5178478642406732 Testing Score: 0.16143088403539396\n",
      " RMSE FOR water_score: Training Score: 19.86684474370558 Testing Score: 26.171654791700334\n",
      " MAE FOR water_score: Training Score: 16.59633055663824 Testing Score: 21.99694056781769\n",
      " r2 FOR water_score: Training Score: 0.5190204246930527 Testing Score: 0.16048671134003922\n",
      " RMSE FOR waste_score: Training Score: 20.263855060104135 Testing Score: 27.495017736691565\n",
      " MAE FOR waste_score: Training Score: 16.975413490993382 Testing Score: 23.386519780788422\n",
      " r2 FOR waste_score: Training Score: 0.49633907046193104 Testing Score: 0.09951739286029271\n",
      " RMSE FOR social_score: Training Score: 19.025335705722377 Testing Score: 24.77174130947039\n",
      " MAE FOR social_score: Training Score: 15.941929835303126 Testing Score: 20.817680648207663\n",
      " r2 FOR social_score: Training Score: 0.558839712750091 Testing Score: 0.26801787752904527\n",
      "#####################################\n",
      "Model lightgbm Training In Progess\n",
      "Training Model lightgbm Complete\n",
      "Prediction For Sub Scores In Progress\n",
      "SubScores:\n",
      " RMSE FOR carbon_score: Training Score: 10.138918475272988 Testing Score: 26.124981597490198\n",
      " MAE FOR carbon_score: Training Score: 8.152769567088157 Testing Score: 21.671842555504846\n",
      " r2 FOR carbon_score: Training Score: 0.8761790072498464 Testing Score: 0.18275976329614785\n",
      " RMSE FOR water_score: Training Score: 9.800277721466308 Testing Score: 26.574588118307656\n",
      " MAE FOR water_score: Training Score: 7.871894424558334 Testing Score: 21.7948492556198\n",
      " r2 FOR water_score: Training Score: 0.8829569569241259 Testing Score: 0.13443777896285536\n",
      " RMSE FOR waste_score: Training Score: 10.141277923732037 Testing Score: 27.59288887512255\n",
      " MAE FOR waste_score: Training Score: 8.17347768082078 Testing Score: 23.05700893466141\n",
      " r2 FOR waste_score: Training Score: 0.8738522550437249 Testing Score: 0.09309527562048547\n",
      " RMSE FOR social_score: Training Score: 9.41836013815905 Testing Score: 24.681962233453213\n",
      " MAE FOR social_score: Training Score: 7.603261288178969 Testing Score: 20.350035754017036\n",
      " r2 FOR social_score: Training Score: 0.8918858995330486 Testing Score: 0.27331404067220066\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'xgboost': XGBRegressor(n_estimators = 1500, learning_rate = 0.01, max_leaves = 10 , n_jobs= -1),\n",
    "    'lightgbm': LGBMRegressor(n_estimators = 1500, max_depth = 10, learning_rate = 0.01, random_state = 42)\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    print ('#####################################')\n",
    "    print(f'Model {name} Training In Progess')\n",
    "    \n",
    "    multi_model = MultiOutputRegressor(model)\n",
    "    multi_model.fit(X_train_final, y_train)\n",
    "    \n",
    "    print (f'Training Model {name} Complete')\n",
    "    print('Prediction For Sub Scores In Progress')\n",
    "\n",
    "    y_pred_train_subscores = multi_model.predict(X_train_final)\n",
    "    y_pred_test_subscores = multi_model.predict(X_test_final)\n",
    "\n",
    "    print('SubScores:')\n",
    "    for i, col in enumerate(y.columns):\n",
    "        y_pred_train_subscores_df = pd.DataFrame(y_pred_train_subscores)\n",
    "        y_pred_test_subscores_df = pd.DataFrame(y_pred_test_subscores)\n",
    "        rmse_train = np.sqrt(MSE(y_train.iloc[ : ,i], y_pred_train_subscores_df.iloc[ : ,i]))\n",
    "        rmse_test = np.sqrt(MSE(y_test.iloc[ : ,i], y_pred_test_subscores_df.iloc[ : ,i]))\n",
    "        mae_train = MAE(y_train.iloc[ : ,i], y_pred_train_subscores_df.iloc[ : ,i])\n",
    "        mae_test = MAE(y_test.iloc[ : ,i], y_pred_test_subscores_df.iloc[ : ,i])\n",
    "        r2_train = R2(y_train.iloc[ : ,i], y_pred_train_subscores_df.iloc[ : ,i])\n",
    "        r2_test = R2(y_test.iloc[ : ,i], y_pred_test_subscores_df.iloc[ : ,i])\n",
    "        print(f\" RMSE FOR {col}: Training Score: {rmse_train} Testing Score: {rmse_test}\")\n",
    "        print(f\" MAE FOR {col}: Training Score: {mae_train} Testing Score: {mae_test}\")\n",
    "        print(f\" r2 FOR {col}: Training Score: {r2_train} Testing Score: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe64f0f-b745-45af-bed1-30707a78d2b5",
   "metadata": {},
   "source": [
    "Using The Predicted SubScores as Features to Predict the EcoScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79cac92c-5756-4f90-b297-3e15d7fd02ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE FOR THE ENTIRE DATASET: 14.70\n",
      " MAE FOR THE ENTIRE DATASET: 10.70\n",
      " R2 FOR THE ENTIRE DATASET: 0.74\n",
      "   carbon_score  water_score  waste_score  social_score\n",
      "0     81.862301    54.271436    37.029578     66.526458\n",
      "1     28.503151    19.713255    74.696942     26.194180\n",
      "###############################################\n",
      "Training Ecoscore With Subscores, Model: xgboost\n",
      "Training Data RMSE: 26.51. Testing Data RMSE: 29.37\n",
      "Training Data MAE: 22.79. Testing Data RMSE: 25.29\n",
      "Training Data R2: 0.16. Testing Data RMSE: -0.01\n",
      "###############################################\n",
      "Training Ecoscore With Subscores, Model: lightgbm\n",
      "Training Data RMSE: 21.99. Testing Data RMSE: 29.95\n",
      "Training Data MAE: 18.69. Testing Data RMSE: 25.63\n",
      "Training Data R2: 0.42. Testing Data RMSE: -0.05\n"
     ]
    }
   ],
   "source": [
    "models_2 = {\n",
    "    'xgboost': XGBRegressor(n_estimators = 1500, learning_rate = 0.01, max_leaves = 10 , n_jobs= -1),\n",
    "    'lightgbm': LGBMRegressor(n_estimators = 1500, max_depth = 10, learning_rate = 0.01, random_state = 42)\n",
    "}\n",
    "\n",
    "X[['country','region','partnership_status','risk_level']] = encoder.transform(X[['country','region','partnership_status','risk_level']])\n",
    "X['last_audit'] = X['last_audit'].str.replace(\"-\",\"\", regex = False)\n",
    "X['last_audit'] = pd.to_numeric(X['last_audit'], errors = 'coerce')\n",
    "X['text_embedding'] = [ast.literal_eval(e) for e in X['text_embedding']]\n",
    "X_embeddings = np.vstack(X['text_embedding'].values).astype(np.float32)\n",
    "X_num = X.drop(columns=['text_embedding']).reset_index(drop=True)\n",
    "X_final = np.hstack([X_num.values, X_embeddings])\n",
    "\n",
    "predicted_subscore = multi_model.predict(X_final)\n",
    "print(f'RMSE FOR THE ENTIRE DATASET: {np.sqrt(MSE(y,predicted_subscore)):.2f}')\n",
    "print(f' MAE FOR THE ENTIRE DATASET: {MAE(y,predicted_subscore):.2f}')\n",
    "print(f' R2 FOR THE ENTIRE DATASET: {R2(y,predicted_subscore):.2f}')\n",
    "subscores = pd.DataFrame(predicted_subscore, columns = ['carbon_score','water_score','waste_score','social_score'])\n",
    "print(subscores.head(2))\n",
    "\n",
    "\n",
    "y_ecoscore = pd.DataFrame(y_ecoscore)\n",
    "X_train_eco,X_test_eco,y_train_eco,y_test_eco = train_test_split(subscores, y_ecoscore, test_size = 0.2, shuffle = True, random_state = 42)\n",
    "\n",
    "for name_2, model_2 in models_2.items():\n",
    "    print('###############################################')\n",
    "    print(f'Training Ecoscore With Subscores, Model: {name_2}')\n",
    "    model_2.fit(X_train_eco,y_train_eco)\n",
    "    ypred_train = model_2.predict(X_train_eco)\n",
    "    ypred_test = model_2.predict(X_test_eco)\n",
    "    RMSE_train_eco = np.sqrt(MSE(y_train_eco,ypred_train))\n",
    "    RMSE_test_eco = np.sqrt(MSE(y_test_eco,ypred_test))\n",
    "    mae_train_eco = MAE(y_train_eco,ypred_train)\n",
    "    mae_test_eco = MAE(y_test_eco,ypred_test)\n",
    "    r2_train_eco = R2(y_train_eco,ypred_train)\n",
    "    r2_test_eco = R2(y_test_eco,ypred_test)\n",
    "    print (f'Training Data RMSE: {RMSE_train_eco:.2f}. Testing Data RMSE: {RMSE_test_eco:.2f}')\n",
    "    print (f'Training Data MAE: {mae_train_eco:.2f}. Testing Data RMSE: {mae_test_eco:.2f}')\n",
    "    print (f'Training Data R2: {r2_train_eco:.2f}. Testing Data RMSE: {r2_test_eco:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1c13304-9ab5-4944-99a8-40d56bc8ebe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 21:49:24,263] A new study created in memory with name: no-name-243c63d9-fd7c-4bdf-912d-0c138fd97770\n",
      "[I 2025-09-08 21:51:37,295] Trial 0 finished with value: 26.507649094674296 and parameters: {'max_depth': 4, 'num_leaves': 153, 'learning_rate': 0.0187326888556976, 'n_estimators': 2322}. Best is trial 0 with value: 26.507649094674296.\n",
      "[I 2025-09-08 21:53:30,662] Trial 1 finished with value: 27.399229177917974 and parameters: {'max_depth': 12, 'num_leaves': 161, 'learning_rate': 0.05737480688642148, 'n_estimators': 2342}. Best is trial 0 with value: 26.507649094674296.\n",
      "[I 2025-09-08 22:01:56,925] Trial 2 finished with value: 27.194435979790505 and parameters: {'max_depth': 15, 'num_leaves': 119, 'learning_rate': 0.0064256680689245265, 'n_estimators': 2128}. Best is trial 0 with value: 26.507649094674296.\n",
      "[I 2025-09-08 22:06:25,116] Trial 3 finished with value: 26.801636268230897 and parameters: {'max_depth': 6, 'num_leaves': 177, 'learning_rate': 0.007244976269847492, 'n_estimators': 3101}. Best is trial 0 with value: 26.507649094674296.\n",
      "[I 2025-09-08 22:13:56,053] Trial 4 finished with value: 26.691729411574684 and parameters: {'max_depth': 5, 'num_leaves': 150, 'learning_rate': 0.004335963203553901, 'n_estimators': 3219}. Best is trial 0 with value: 26.507649094674296.\n",
      "[I 2025-09-08 22:28:46,495] Trial 5 finished with value: 27.18558837321869 and parameters: {'max_depth': 14, 'num_leaves': 129, 'learning_rate': 0.003798768295469026, 'n_estimators': 2519}. Best is trial 0 with value: 26.507649094674296.\n",
      "[I 2025-09-08 22:30:02,124] Trial 6 finished with value: 26.79154293763835 and parameters: {'max_depth': 6, 'num_leaves': 141, 'learning_rate': 0.04935200127435126, 'n_estimators': 3135}. Best is trial 0 with value: 26.507649094674296.\n",
      "[I 2025-09-08 22:35:36,986] Trial 7 finished with value: 26.448147009638117 and parameters: {'max_depth': 3, 'num_leaves': 148, 'learning_rate': 0.005386063844104785, 'n_estimators': 3566}. Best is trial 7 with value: 26.448147009638117.\n",
      "[I 2025-09-08 23:09:02,494] Trial 8 finished with value: 27.22412407462338 and parameters: {'max_depth': 15, 'num_leaves': 166, 'learning_rate': 0.0011525184251895532, 'n_estimators': 1647}. Best is trial 7 with value: 26.448147009638117.\n",
      "[I 2025-09-08 23:25:58,494] Trial 9 finished with value: 27.198862742717505 and parameters: {'max_depth': 15, 'num_leaves': 151, 'learning_rate': 0.003400644675190753, 'n_estimators': 1204}. Best is trial 7 with value: 26.448147009638117.\n",
      "[I 2025-09-08 23:29:39,979] Trial 10 finished with value: 26.98392640865026 and parameters: {'max_depth': 9, 'num_leaves': 57, 'learning_rate': 0.019197418262701094, 'n_estimators': 3978}. Best is trial 7 with value: 26.448147009638117.\n",
      "[I 2025-09-08 23:32:15,758] Trial 11 finished with value: 26.45844111529307 and parameters: {'max_depth': 3, 'num_leaves': 85, 'learning_rate': 0.016317670634524106, 'n_estimators': 3973}. Best is trial 7 with value: 26.448147009638117.\n",
      "[I 2025-09-08 23:34:22,636] Trial 12 finished with value: 26.427809189790924 and parameters: {'max_depth': 3, 'num_leaves': 81, 'learning_rate': 0.02022349128653766, 'n_estimators': 3956}. Best is trial 12 with value: 26.427809189790924.\n",
      "[I 2025-09-09 00:00:03,744] Trial 13 finished with value: 27.04555932119027 and parameters: {'max_depth': 9, 'num_leaves': 199, 'learning_rate': 0.0015595597407755148, 'n_estimators': 3536}. Best is trial 12 with value: 26.427809189790924.\n",
      "[I 2025-09-09 00:01:36,776] Trial 14 finished with value: 26.420131675474376 and parameters: {'max_depth': 3, 'num_leaves': 24, 'learning_rate': 0.029251618790681614, 'n_estimators': 3613}. Best is trial 14 with value: 26.420131675474376.\n",
      "[I 2025-09-09 00:02:34,162] Trial 15 finished with value: 26.75280147602242 and parameters: {'max_depth': 7, 'num_leaves': 21, 'learning_rate': 0.09676228093223391, 'n_estimators': 3560}. Best is trial 14 with value: 26.420131675474376.\n",
      "[I 2025-09-09 00:05:13,606] Trial 16 finished with value: 26.861465362692293 and parameters: {'max_depth': 11, 'num_leaves': 41, 'learning_rate': 0.03316158803860498, 'n_estimators': 2882}. Best is trial 14 with value: 26.420131675474376.\n",
      "[I 2025-09-09 00:09:26,417] Trial 17 finished with value: 26.896431198429244 and parameters: {'max_depth': 7, 'num_leaves': 96, 'learning_rate': 0.01165169141760531, 'n_estimators': 3733}. Best is trial 14 with value: 26.420131675474376.\n",
      "[I 2025-09-09 00:11:02,582] Trial 18 finished with value: 26.50521151091341 and parameters: {'max_depth': 4, 'num_leaves': 68, 'learning_rate': 0.038196153119997155, 'n_estimators': 2823}. Best is trial 14 with value: 26.420131675474376.\n",
      "[I 2025-09-09 00:14:03,309] Trial 19 finished with value: 26.6349471852954 and parameters: {'max_depth': 9, 'num_leaves': 23, 'learning_rate': 0.026908396785097346, 'n_estimators': 3297}. Best is trial 14 with value: 26.420131675474376.\n",
      "[I 2025-09-09 00:14:52,246] Trial 20 finished with value: 26.548729360571723 and parameters: {'max_depth': 3, 'num_leaves': 49, 'learning_rate': 0.09357387640899283, 'n_estimators': 1859}. Best is trial 14 with value: 26.420131675474376.\n",
      "[I 2025-09-09 00:18:34,118] Trial 21 finished with value: 26.41174527277477 and parameters: {'max_depth': 3, 'num_leaves': 105, 'learning_rate': 0.010974284448002799, 'n_estimators': 3643}. Best is trial 21 with value: 26.41174527277477.\n",
      "[I 2025-09-09 00:21:11,938] Trial 22 finished with value: 26.71710942869957 and parameters: {'max_depth': 5, 'num_leaves': 103, 'learning_rate': 0.014481681199055378, 'n_estimators': 3791}. Best is trial 21 with value: 26.41174527277477.\n",
      "[I 2025-09-09 00:25:14,466] Trial 23 finished with value: 26.512139994417776 and parameters: {'max_depth': 4, 'num_leaves': 78, 'learning_rate': 0.00912577884429533, 'n_estimators': 3389}. Best is trial 21 with value: 26.41174527277477.\n",
      "[I 2025-09-09 00:26:47,421] Trial 24 finished with value: 26.648705061244765 and parameters: {'max_depth': 5, 'num_leaves': 118, 'learning_rate': 0.0265489569941639, 'n_estimators': 3790}. Best is trial 21 with value: 26.41174527277477.\n",
      "[I 2025-09-09 00:30:40,928] Trial 25 finished with value: 26.457295828806135 and parameters: {'max_depth': 3, 'num_leaves': 36, 'learning_rate': 0.008938304448719653, 'n_estimators': 2779}. Best is trial 21 with value: 26.41174527277477.\n",
      "[I 2025-09-09 00:31:46,218] Trial 26 finished with value: 26.98850637297276 and parameters: {'max_depth': 7, 'num_leaves': 92, 'learning_rate': 0.061348890972569695, 'n_estimators': 3997}. Best is trial 21 with value: 26.41174527277477.\n",
      "[I 2025-09-09 00:33:31,448] Trial 27 finished with value: 26.510468598894875 and parameters: {'max_depth': 4, 'num_leaves': 59, 'learning_rate': 0.024248024176028997, 'n_estimators': 3701}. Best is trial 21 with value: 26.41174527277477.\n",
      "[I 2025-09-09 00:37:09,422] Trial 28 finished with value: 26.764192968144332 and parameters: {'max_depth': 6, 'num_leaves': 79, 'learning_rate': 0.011453206358952726, 'n_estimators': 3006}. Best is trial 21 with value: 26.41174527277477.\n",
      "[I 2025-09-09 00:50:26,311] Trial 29 finished with value: 26.67637382569706 and parameters: {'max_depth': 5, 'num_leaves': 111, 'learning_rate': 0.0023650061249552963, 'n_estimators': 3427}. Best is trial 21 with value: 26.41174527277477.\n",
      "[I 2025-09-09 00:52:21,970] Trial 30 finished with value: 26.38271038827916 and parameters: {'max_depth': 3, 'num_leaves': 134, 'learning_rate': 0.01996992499390305, 'n_estimators': 2638}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 00:54:18,224] Trial 31 finished with value: 26.443264328377786 and parameters: {'max_depth': 3, 'num_leaves': 133, 'learning_rate': 0.019343392401588437, 'n_estimators': 2610}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 00:55:27,824] Trial 32 finished with value: 26.537623833652344 and parameters: {'max_depth': 4, 'num_leaves': 107, 'learning_rate': 0.039234785878428276, 'n_estimators': 2100}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 00:57:24,459] Trial 33 finished with value: 26.473961127125346 and parameters: {'max_depth': 3, 'num_leaves': 128, 'learning_rate': 0.014118376923581751, 'n_estimators': 1109}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 00:59:05,611] Trial 34 finished with value: 26.508197657982123 and parameters: {'max_depth': 4, 'num_leaves': 66, 'learning_rate': 0.02211200231289422, 'n_estimators': 2366}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:07:15,182] Trial 35 finished with value: 27.12001893277137 and parameters: {'max_depth': 11, 'num_leaves': 120, 'learning_rate': 0.0073226963388980165, 'n_estimators': 3332}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:08:20,120] Trial 36 finished with value: 26.96622021728124 and parameters: {'max_depth': 6, 'num_leaves': 96, 'learning_rate': 0.06505936170848217, 'n_estimators': 1530}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:09:44,685] Trial 37 finished with value: 26.697728784251407 and parameters: {'max_depth': 5, 'num_leaves': 163, 'learning_rate': 0.0320990826918983, 'n_estimators': 3076}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:12:17,744] Trial 38 finished with value: 26.55689677110526 and parameters: {'max_depth': 4, 'num_leaves': 176, 'learning_rate': 0.015568543117110839, 'n_estimators': 2099}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:13:29,324] Trial 39 finished with value: 26.85110494960433 and parameters: {'max_depth': 8, 'num_leaves': 37, 'learning_rate': 0.04919615182475186, 'n_estimators': 3616}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:16:29,035] Trial 40 finished with value: 26.454403319378844 and parameters: {'max_depth': 3, 'num_leaves': 137, 'learning_rate': 0.011291809589222363, 'n_estimators': 3841}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:18:36,308] Trial 41 finished with value: 26.464617753108534 and parameters: {'max_depth': 3, 'num_leaves': 135, 'learning_rate': 0.019375657355076435, 'n_estimators': 2369}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:23:06,387] Trial 42 finished with value: 26.492470039559397 and parameters: {'max_depth': 3, 'num_leaves': 127, 'learning_rate': 0.005752316897278669, 'n_estimators': 2678}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:25:01,464] Trial 43 finished with value: 26.532016087169932 and parameters: {'max_depth': 4, 'num_leaves': 144, 'learning_rate': 0.02023108174876447, 'n_estimators': 2572}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:26:40,355] Trial 44 finished with value: 26.51138968086784 and parameters: {'max_depth': 4, 'num_leaves': 157, 'learning_rate': 0.02865173922054127, 'n_estimators': 3162}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:29:42,178] Trial 45 finished with value: 26.69843334632053 and parameters: {'max_depth': 5, 'num_leaves': 113, 'learning_rate': 0.016689620893268805, 'n_estimators': 2277}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:35:37,109] Trial 46 finished with value: 26.4328775558484 and parameters: {'max_depth': 3, 'num_leaves': 122, 'learning_rate': 0.007405853298221708, 'n_estimators': 2993}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:43:29,276] Trial 47 finished with value: 27.133639246761245 and parameters: {'max_depth': 14, 'num_leaves': 122, 'learning_rate': 0.007603217896206946, 'n_estimators': 2988}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:47:09,569] Trial 48 finished with value: 26.75398111169417 and parameters: {'max_depth': 6, 'num_leaves': 104, 'learning_rate': 0.01256483407016048, 'n_estimators': 3524}. Best is trial 30 with value: 26.38271038827916.\n",
      "[I 2025-09-09 01:54:06,909] Trial 49 finished with value: 26.494820697802098 and parameters: {'max_depth': 3, 'num_leaves': 91, 'learning_rate': 0.0040487117602100555, 'n_estimators': 3866}. Best is trial 30 with value: 26.38271038827916.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Trial:\n",
      "Value: 26.38271038827916\n",
      "Params: {'max_depth': 3, 'num_leaves': 134, 'learning_rate': 0.01996992499390305, 'n_estimators': 2638}\n"
     ]
    }
   ],
   "source": [
    "X_train_eco_1,X_val_eco_1,y_train_eco_1,y_val_eco_1 = train_test_split(X_train_final, y_train, random_state = 42, test_size = 0.2, shuffle = True)\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1000, 4000)\n",
    "    }\n",
    "\n",
    "    preds_list = []\n",
    "\n",
    "    # Train one model per target column\n",
    "    for i in range(y_train_eco_1.shape[1]):\n",
    "        model = LGBMRegressor(**param)\n",
    "        model.fit(\n",
    "        X_train_eco_1, y_train_eco_1.iloc[:, i].to_numpy(),\n",
    "        eval_set=[(X_val_eco_1, y_val_eco_1.iloc[:, i].to_numpy())],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "        )\n",
    "\n",
    "        preds_list.append(model.predict(X_test_final))\n",
    "\n",
    "    preds = np.vstack(preds_list).T  # shape (n_samples, n_targets)\n",
    "    rmse_pred = np.sqrt(MSE(y_test, preds))\n",
    "\n",
    "    return rmse_pred\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction = 'minimize') # Minimse RMSE\n",
    "study.optimize(objective, n_trials = 50)\n",
    "print('Best Trial:')\n",
    "trial = study.best_trial\n",
    "print(f'Value:', trial.value)\n",
    "print(f'Params:',trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cddaa446-9fe8-459c-aba1-18f2774dc4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubScores:\n",
      " RMSE FOR carbon_score: Testing Score: 25.84\n",
      " MAE FOR carbon_score: Testing Score: 21.67\n",
      " r2 FOR carbon_score: Testing Score: 0.20\n",
      " RMSE FOR water_score: Testing Score: 26.25\n",
      " MAE FOR water_score: Testing Score: 21.68\n",
      " r2 FOR water_score: Testing Score: 0.16\n",
      " RMSE FOR waste_score: Testing Score: 27.45\n",
      " MAE FOR waste_score: Testing Score: 23.02\n",
      " r2 FOR waste_score: Testing Score: 0.10\n",
      " RMSE FOR social_score: Testing Score: 24.26\n",
      " MAE FOR social_score: Testing Score: 20.12\n",
      " r2 FOR social_score: Testing Score: 0.30\n"
     ]
    }
   ],
   "source": [
    "model_3 = LGBMRegressor(max_depth= 3, num_leaves = 134, learning_rate =  0.01996992499390305, n_estimators = 2638, objective = 'regression' ,\n",
    "        metric = 'rmse', verbosity = -1, boosting_type = 'gbdt', random_state = 42)\n",
    "\n",
    "multi_model_2 = MultiOutputRegressor(model_3)\n",
    "\n",
    "multi_model_2.fit(X_train_final, y_train)\n",
    "\n",
    "preds_2 = multi_model_2.predict(X_test_final)\n",
    "\n",
    "print('SubScores:')\n",
    "for i, col in enumerate(y.columns):\n",
    "    pred_test_subscores_df = pd.DataFrame(preds_2)\n",
    "    rmse_test = np.sqrt(MSE(y_test.iloc[ : ,i], pred_test_subscores_df.iloc[ : ,i]))\n",
    "    mae_test = MAE(y_test.iloc[ : ,i], pred_test_subscores_df.iloc[ : ,i])\n",
    "    r2_test = R2(y_test.iloc[ : ,i], pred_test_subscores_df.iloc[ : ,i])\n",
    "    print(f\" RMSE FOR {col}: Testing Score: {rmse_test:.2f}\")\n",
    "    print(f\" MAE FOR {col}: Testing Score: {mae_test:.2f}\")\n",
    "    print(f\" r2 FOR {col}: Testing Score: {r2_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87afff36-ba72-42ee-97bd-ae92d54762ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-09 02:14:04,429] A new study created in memory with name: no-name-78b8d895-f878-43d5-b758-416cca42a3f6\n",
      "[I 2025-09-09 02:14:20,316] Trial 0 finished with value: 33.04280527109028 and parameters: {'max_depth': 13, 'num_leaves': 56, 'learning_rate': 0.035704246891821785, 'n_estimators': 3285}. Best is trial 0 with value: 33.04280527109028.\n",
      "[I 2025-09-09 02:14:26,704] Trial 1 finished with value: 32.0327996354399 and parameters: {'max_depth': 7, 'num_leaves': 198, 'learning_rate': 0.04212510650325252, 'n_estimators': 2470}. Best is trial 1 with value: 32.0327996354399.\n",
      "[I 2025-09-09 02:14:37,214] Trial 2 finished with value: 32.248628497186665 and parameters: {'max_depth': 13, 'num_leaves': 41, 'learning_rate': 0.02989118758888972, 'n_estimators': 2635}. Best is trial 1 with value: 32.0327996354399.\n",
      "[I 2025-09-09 02:14:43,608] Trial 3 finished with value: 29.47440349254141 and parameters: {'max_depth': 8, 'num_leaves': 25, 'learning_rate': 0.00234599652421054, 'n_estimators': 2112}. Best is trial 3 with value: 29.47440349254141.\n",
      "[I 2025-09-09 02:14:48,914] Trial 4 finished with value: 29.958435005420824 and parameters: {'max_depth': 14, 'num_leaves': 23, 'learning_rate': 0.009334311042455754, 'n_estimators': 1997}. Best is trial 3 with value: 29.47440349254141.\n",
      "[I 2025-09-09 02:14:58,885] Trial 5 finished with value: 33.57409712814838 and parameters: {'max_depth': 7, 'num_leaves': 148, 'learning_rate': 0.09104805673426815, 'n_estimators': 3377}. Best is trial 3 with value: 29.47440349254141.\n",
      "[I 2025-09-09 02:15:10,106] Trial 6 finished with value: 32.2855597573342 and parameters: {'max_depth': 15, 'num_leaves': 143, 'learning_rate': 0.027047563642095165, 'n_estimators': 1827}. Best is trial 3 with value: 29.47440349254141.\n",
      "[I 2025-09-09 02:15:13,090] Trial 7 finished with value: 30.07211014007324 and parameters: {'max_depth': 8, 'num_leaves': 23, 'learning_rate': 0.014082073824151373, 'n_estimators': 1281}. Best is trial 3 with value: 29.47440349254141.\n",
      "[I 2025-09-09 02:15:20,263] Trial 8 finished with value: 31.20652029510861 and parameters: {'max_depth': 5, 'num_leaves': 127, 'learning_rate': 0.02527466393467896, 'n_estimators': 3329}. Best is trial 3 with value: 29.47440349254141.\n",
      "[I 2025-09-09 02:15:32,481] Trial 9 finished with value: 32.70736121333182 and parameters: {'max_depth': 14, 'num_leaves': 114, 'learning_rate': 0.03341618773901575, 'n_estimators': 2222}. Best is trial 3 with value: 29.47440349254141.\n",
      "[I 2025-09-09 02:15:34,104] Trial 10 finished with value: 29.256543337581604 and parameters: {'max_depth': 3, 'num_leaves': 82, 'learning_rate': 0.001654567100877028, 'n_estimators': 1288}. Best is trial 10 with value: 29.256543337581604.\n",
      "[I 2025-09-09 02:15:35,551] Trial 11 finished with value: 29.242680071885747 and parameters: {'max_depth': 3, 'num_leaves': 74, 'learning_rate': 0.001614909701469434, 'n_estimators': 1057}. Best is trial 11 with value: 29.242680071885747.\n",
      "[I 2025-09-09 02:15:37,048] Trial 12 finished with value: 29.23684992403832 and parameters: {'max_depth': 3, 'num_leaves': 74, 'learning_rate': 0.0012870919267741528, 'n_estimators': 1192}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:15:38,288] Trial 13 finished with value: 29.30588421812177 and parameters: {'max_depth': 3, 'num_leaves': 78, 'learning_rate': 0.0035641974595186777, 'n_estimators': 1229}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:15:42,455] Trial 14 finished with value: 29.29708481040048 and parameters: {'max_depth': 5, 'num_leaves': 85, 'learning_rate': 0.0011113637815462069, 'n_estimators': 1532}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:15:47,214] Trial 15 finished with value: 29.69857318297462 and parameters: {'max_depth': 11, 'num_leaves': 63, 'learning_rate': 0.004717112342469368, 'n_estimators': 1102}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:15:55,122] Trial 16 finished with value: 29.348578955680857 and parameters: {'max_depth': 5, 'num_leaves': 96, 'learning_rate': 0.001115229866669209, 'n_estimators': 3804}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:15:57,143] Trial 17 finished with value: 29.396358730951384 and parameters: {'max_depth': 3, 'num_leaves': 62, 'learning_rate': 0.004992348319177008, 'n_estimators': 1685}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:01,900] Trial 18 finished with value: 29.4558309957082 and parameters: {'max_depth': 10, 'num_leaves': 104, 'learning_rate': 0.0021946930756854112, 'n_estimators': 1057}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:05,567] Trial 19 finished with value: 29.611828292342516 and parameters: {'max_depth': 5, 'num_leaves': 193, 'learning_rate': 0.008590168344786389, 'n_estimators': 1467}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:09,907] Trial 20 finished with value: 29.48170421575777 and parameters: {'max_depth': 4, 'num_leaves': 49, 'learning_rate': 0.003110556028910797, 'n_estimators': 2889}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:10,960] Trial 21 finished with value: 29.242114678599116 and parameters: {'max_depth': 3, 'num_leaves': 78, 'learning_rate': 0.0016594253951384328, 'n_estimators': 1015}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:12,590] Trial 22 finished with value: 29.259870177175916 and parameters: {'max_depth': 4, 'num_leaves': 73, 'learning_rate': 0.0016030853714950792, 'n_estimators': 1016}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:18,004] Trial 23 finished with value: 29.418593352460938 and parameters: {'max_depth': 6, 'num_leaves': 96, 'learning_rate': 0.001524374610876868, 'n_estimators': 1665}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:20,547] Trial 24 finished with value: 29.25744634318474 and parameters: {'max_depth': 4, 'num_leaves': 122, 'learning_rate': 0.001026848016273429, 'n_estimators': 1495}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:21,962] Trial 25 finished with value: 29.38343945884644 and parameters: {'max_depth': 3, 'num_leaves': 36, 'learning_rate': 0.005705738823116693, 'n_estimators': 1350}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:27,220] Trial 26 finished with value: 29.439455557705834 and parameters: {'max_depth': 6, 'num_leaves': 72, 'learning_rate': 0.0021710273199100124, 'n_estimators': 1844}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:28,900] Trial 27 finished with value: 29.306954963298935 and parameters: {'max_depth': 4, 'num_leaves': 93, 'learning_rate': 0.003029203746098673, 'n_estimators': 1061}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:34,527] Trial 28 finished with value: 29.453144057104943 and parameters: {'max_depth': 6, 'num_leaves': 48, 'learning_rate': 0.0014563412998504003, 'n_estimators': 2408}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:52,031] Trial 29 finished with value: 30.91445755899058 and parameters: {'max_depth': 11, 'num_leaves': 64, 'learning_rate': 0.006941802777559791, 'n_estimators': 3961}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:55,332] Trial 30 finished with value: 29.7036975018922 and parameters: {'max_depth': 3, 'num_leaves': 110, 'learning_rate': 0.017569013395865092, 'n_estimators': 2749}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:56,748] Trial 31 finished with value: 29.258631237139685 and parameters: {'max_depth': 3, 'num_leaves': 85, 'learning_rate': 0.0017286877797255712, 'n_estimators': 1283}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:16:58,554] Trial 32 finished with value: 29.303477200193104 and parameters: {'max_depth': 4, 'num_leaves': 80, 'learning_rate': 0.002575590056557157, 'n_estimators': 1195}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:00,309] Trial 33 finished with value: 29.330114213182412 and parameters: {'max_depth': 3, 'num_leaves': 53, 'learning_rate': 0.003930711367423777, 'n_estimators': 1408}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:03,636] Trial 34 finished with value: 29.30245166817418 and parameters: {'max_depth': 4, 'num_leaves': 38, 'learning_rate': 0.0018930490244690244, 'n_estimators': 1608}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:06,911] Trial 35 finished with value: 29.34237626938205 and parameters: {'max_depth': 7, 'num_leaves': 69, 'learning_rate': 0.0012595342449853776, 'n_estimators': 1003}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:09,720] Trial 36 finished with value: 31.26174782314405 and parameters: {'max_depth': 6, 'num_leaves': 95, 'learning_rate': 0.06623449585379702, 'n_estimators': 1249}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:17,006] Trial 37 finished with value: 29.38870054530005 and parameters: {'max_depth': 8, 'num_leaves': 57, 'learning_rate': 0.0014175526996912486, 'n_estimators': 1816}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:22,805] Trial 38 finished with value: 29.472058935124405 and parameters: {'max_depth': 5, 'num_leaves': 139, 'learning_rate': 0.002646167138639988, 'n_estimators': 3098}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:26,134] Trial 39 finished with value: 29.29736204676277 and parameters: {'max_depth': 3, 'num_leaves': 86, 'learning_rate': 0.0018476796946060338, 'n_estimators': 2168}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:29,277] Trial 40 finished with value: 29.44866371422813 and parameters: {'max_depth': 4, 'num_leaves': 104, 'learning_rate': 0.004044470498178575, 'n_estimators': 2015}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:31,642] Trial 41 finished with value: 29.256290229488727 and parameters: {'max_depth': 4, 'num_leaves': 121, 'learning_rate': 0.0010421850582002406, 'n_estimators': 1447}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:33,296] Trial 42 finished with value: 29.24235931174456 and parameters: {'max_depth': 3, 'num_leaves': 162, 'learning_rate': 0.0012302375254113062, 'n_estimators': 1381}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:35,831] Trial 43 finished with value: 29.281407517537538 and parameters: {'max_depth': 5, 'num_leaves': 171, 'learning_rate': 0.0010305750964516266, 'n_estimators': 1175}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:39,023] Trial 44 finished with value: 29.26352960051098 and parameters: {'max_depth': 4, 'num_leaves': 172, 'learning_rate': 0.001254275935150066, 'n_estimators': 1384}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:41,095] Trial 45 finished with value: 29.28476457191256 and parameters: {'max_depth': 3, 'num_leaves': 157, 'learning_rate': 0.002051343291396551, 'n_estimators': 1716}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:43,901] Trial 46 finished with value: 29.277843808408118 and parameters: {'max_depth': 3, 'num_leaves': 126, 'learning_rate': 0.0013434904165488437, 'n_estimators': 2340}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:46,306] Trial 47 finished with value: 29.28464530241282 and parameters: {'max_depth': 5, 'num_leaves': 153, 'learning_rate': 0.0012406876732439492, 'n_estimators': 1149}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:17:55,104] Trial 48 finished with value: 29.606823177822474 and parameters: {'max_depth': 12, 'num_leaves': 117, 'learning_rate': 0.002506771378199057, 'n_estimators': 1501}. Best is trial 12 with value: 29.23684992403832.\n",
      "[I 2025-09-09 02:18:01,734] Trial 49 finished with value: 29.371691909020008 and parameters: {'max_depth': 7, 'num_leaves': 136, 'learning_rate': 0.001026249128876525, 'n_estimators': 1911}. Best is trial 12 with value: 29.23684992403832.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Trial:\n",
      "Value: 29.23684992403832\n",
      "Params: {'max_depth': 3, 'num_leaves': 74, 'learning_rate': 0.0012870919267741528, 'n_estimators': 1192}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1000, 4000)\n",
    "    }\n",
    "\n",
    "    model = LGBMRegressor(**param)\n",
    "    model.fit(X_train_eco,y_train_eco)\n",
    "\n",
    "    preds = model.predict(X_test_eco)\n",
    "    \n",
    "    rmse_pred = np.sqrt(MSE(y_test_eco, preds))\n",
    "\n",
    "    return rmse_pred\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction = 'minimize') # Minimse RMSE\n",
    "study.optimize(objective, n_trials = 50)\n",
    "print('Best Trial:')\n",
    "trial = study.best_trial\n",
    "print(f'Value:', trial.value)\n",
    "print(f'Params:',trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38ae4361-0d38-41f6-990b-a4e9a618d95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eco Score Metrics:\n",
      "RMSE: 29.270555127650308\n",
      "MAE: 25.284544562264855\n",
      "r2: -0.006087750161555938\n"
     ]
    }
   ],
   "source": [
    "model_4 = LGBMRegressor(max_depth = 3, num_leaves = 74, learning_rate = 0.0012870919267741528, n_estimators =  1192, objective =  'regression',\n",
    "        metric = 'rmse',verbosity = -1, boosting_type = 'gbdt')\n",
    "\n",
    "model_4.fit(X_train_eco,y_train_eco)\n",
    "\n",
    "preds_3 = model_4.predict(X_test_eco)\n",
    "\n",
    "rmse = np.sqrt(MSE(y_test_eco,preds_3))\n",
    "mae = MAE(y_test_eco,preds_3)\n",
    "r2 = R2(y_test_eco,preds_3)\n",
    "\n",
    "print(' Eco Score Metrics:')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'r2: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df0902-8c9a-40ee-b7c3-b2c7d068e7b3",
   "metadata": {},
   "source": [
    "Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20a94c30-1785-4647-9f37-10b2bf52d0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/Encoder_V1.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(multi_model_2, '../models/SubScores_V1.pkl')\n",
    "joblib.dump(model_4, '../models/Ecoscore_V1.pkl')\n",
    "joblib.dump(encoder, '../models/Encoder_V1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d2177-daea-475d-90fe-a4482b558ec7",
   "metadata": {},
   "source": [
    "THE DATASET IS AI GENERATED AND HAS LITTLE SIGNIFICANT RELATIONSHIP, SPENDING TIME AND COMPUTE ON THE DATASET IS NOT NECESSARY AS THE DATASET IS ONLY USED TO SHOW A WORKING PROTOTYPE OF THE ECOCHAIN AI APP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (churn)",
   "language": "python",
   "name": "churn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
